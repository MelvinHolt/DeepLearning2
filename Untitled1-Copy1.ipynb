{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Activation, Dropout, Lambda\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza = pd.read_csv('ingredients_labeled.csv')\n",
    "CLASSES = pizza['ingredient'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (100, 100, 3)\n",
    "NUM_CLASSES =  1095\n",
    "LEARNING_RATE = 0.1\n",
    "PATIENCE = 3\n",
    "VERBOSE = 1\n",
    "LEARNING_RATE_REDUCTION_FACTOR = 0.5\n",
    "MIN_LEARNING_RATE = 0.00001\n",
    "\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "MODEL_OUT_DIR = ''\n",
    "OUTPUT_DIR = 'output'\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled = pd.read_csv('data/train_labeled.csv').dropna()\n",
    "test_labeled = pd.read_csv('data/test_labeled.csv').dropna()\n",
    "val_labeled = pd.read_csv('data/val_labeled.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled['ingredients_cleaned'] = train_labeled['ingredients_cleaned'].apply(lambda x:x.split(\",\"))\n",
    "test_labeled['ingredients_cleaned'] = test_labeled['ingredients_cleaned'].apply(lambda x:x.split(\",\"))\n",
    "val_labeled['ingredients_cleaned'] = val_labeled['ingredients_cleaned'].apply(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_process(x):\n",
    "    import tensorflow as tf\n",
    "    hsv = tf.image.rgb_to_hsv(x)\n",
    "    gray = tf.image.rgb_to_grayscale(x)\n",
    "    rez = tf.concat([hsv, gray], axis=-1)\n",
    "    return rez\n",
    "\n",
    "def network(input_shape, num_classes):\n",
    "    img_input = Input(shape=input_shape, name='data')\n",
    "    x = Lambda(image_process)(img_input)\n",
    "    x = Conv2D(16, (5, 5), strides=(1, 1), padding='same', name='conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool1')(x)\n",
    "    x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', name='conv2')(x)\n",
    "    x = Activation('relu', name='conv2_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool2')(x)\n",
    "    x = Conv2D(64, (5, 5), strides=(1, 1), padding='same', name='conv3')(x)\n",
    "    x = Activation('relu', name='conv3_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool3')(x)\n",
    "    x = Conv2D(128, (5, 5), strides=(1, 1), padding='same', name='conv4')(x)\n",
    "    x = Activation('relu', name='conv4_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool4')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu', name='fcl1')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu', name='fcl2')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(num_classes, activation='sigmoid', name='predictions')(x)\n",
    "    rez = Model(inputs=img_input, outputs=out)\n",
    "    return rez\n",
    "\n",
    "def resnet(input_shape, num_classes):\n",
    "    resnet = ResNet50(weights='imagenet',\n",
    "                       input_shape=input_shape,\n",
    "                   include_top=False)\n",
    "    image = resnet.get_layer(index=0).output\n",
    "    output = resnet.get_layer(index=-1).output\n",
    "    output = Flatten()(output)\n",
    "    output = Dense(num_classes, activation='sigmoid', name='predictions')(output)\n",
    "    \n",
    "    resnet = Model(inputs = image, outputs = output)\n",
    "    return resnet\n",
    "    \n",
    "    \n",
    "# model = network(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "model = network(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "resnet = resnet(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restnet = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = model.get_layer(index=0).output\n",
    "# # x= model.get_layer('avg_pool').output\n",
    "# output = model.get_layer(index=-1).output\n",
    "# output = Flatten()(output)\n",
    "# output = Dense(NUM_CLASSES, activation='softmax', name='predictions')(output)\n",
    "\n",
    "# restnet = Model(inputs = model.input, outputs = output)\n",
    "# model = restnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3405 validated image filenames belonging to 1095 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.0,\n",
    "        height_shift_range=0.0,\n",
    "        zoom_range=0.0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,  # randomly flip images\n",
    ")\n",
    "\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_labeled,\n",
    "    directory=\"./Recipes5k/images/\",\n",
    "    x_col=\"url\",\n",
    "    y_col=\"ingredients_cleaned\",\n",
    "    batch_size=32,\n",
    "    seed=SEED,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    classes=CLASSES,\n",
    "    target_size=(100,100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 783 validated image filenames belonging to 1095 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_labeled,\n",
    "    directory=\"./Recipes5k/images/\",\n",
    "    x_col=\"url\",\n",
    "    y_col=\"ingredients_cleaned\",\n",
    "    batch_size=32,\n",
    "    seed=SEED,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",\n",
    "    classes=CLASSES,\n",
    "    target_size=(100,100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.0,\n",
    "        height_shift_range=0.0,\n",
    "        zoom_range=0.0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,  # randomly flip images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 633 validated image filenames belonging to 1095 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_labeled,\n",
    "    directory=\"./Recipes5k/images/\",\n",
    "    x_col=\"url\",\n",
    "    y_col=\"ingredients_cleaned\",\n",
    "    batch_size=32,\n",
    "    seed=SEED,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",\n",
    "    classes=CLASSES,\n",
    "    target_size=(100,100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_dir = os.path.join(OUTPUT_DIR, \"test\")\n",
    "                            \n",
    "if not os.path.exists(model_out_dir):\n",
    "    os.makedirs(model_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vegetarian        2149\n",
       "Non-Vegetarian    1256\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labeled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_metrics(pred_list,\n",
    "                       verbose,\n",
    "#                        extra_vars,\n",
    "#                        split,\n",
    "                       **kwargs):\n",
    "    print(pred_list)\n",
    "    print(verbose)\n",
    "    \"\"\"\n",
    "    Multiclass classification metrics. see multilabel ranking metrics in sklearn library for more info:\n",
    "        http://scikit-learn.org/stable/modules/model_evaluation.html#multilabel-ranking-metrics\n",
    "    :param pred_list: dictionary of hypothesis sentences\n",
    "    :param verbose: if greater than 0 the metric measures are printed out\n",
    "    :param extra_vars: extra variables\n",
    "                        extra_vars['word2idx'] - dictionary mapping from words to indices\n",
    "                        extra_vars['references'] - list of GT labels\n",
    "    :param split: split on which we are evaluating\n",
    "    :return: Dictionary of multilabel metrics\n",
    "    \"\"\"\n",
    "    from sklearn import metrics as sklearn_metrics\n",
    "\n",
    "#     word2idx = extra_vars[split]['word2idx']\n",
    "\n",
    "#     # check if an additional dictionary matching raw to basic and general labels is provided\n",
    "#     # in that case a more general evaluation will be considered\n",
    "#     raw2basic = extra_vars[split].get('raw2basic', None)\n",
    "#     if raw2basic is not None:\n",
    "#         logger.info('Applying general evaluation with raw2basic dictionary.')\n",
    "\n",
    "#     if raw2basic is None:\n",
    "#         n_classes = len(word2idx)\n",
    "#     else:\n",
    "#         basic_values = set(raw2basic.values())\n",
    "#         n_classes = len(basic_values)\n",
    "    \n",
    "    n_samples = 32\n",
    "    print(\"SHAPE\", n_samples)\n",
    "    n_classes = 1095\n",
    "\n",
    "    # Create prediction matrix\n",
    "    y_pred = np.zeros((n_samples, n_classes))\n",
    "    for i_s, sample in list(enumerate(pred_list)):\n",
    "        for word in sample:\n",
    "            if raw2basic is None:\n",
    "                y_pred[i_s, word2idx[word]] = 1\n",
    "            else:\n",
    "                word = word.strip()\n",
    "                y_pred[i_s, raw2basic[word]] = 1\n",
    "\n",
    "    # Prepare GT\n",
    "    gt_list = extra_vars[split]['references']\n",
    "\n",
    "    if raw2basic is None:\n",
    "        y_gt = np.array(gt_list)\n",
    "    else:\n",
    "        idx2word = {v: k for k, v in iteritems(word2idx)}\n",
    "        y_gt = np.zeros((n_samples, n_classes))\n",
    "        for i_s, sample in list(enumerate(gt_list)):\n",
    "            for raw_idx, is_active in list(enumerate(sample)):\n",
    "                if is_active:\n",
    "                    word = idx2word[raw_idx].strip()\n",
    "                    y_gt[i_s, raw2basic[word]] = 1\n",
    "\n",
    "    # Compute Coverage Error\n",
    "    coverr = sklearn_metrics.coverage_error(y_gt, y_pred)\n",
    "    # Compute Label Ranking AvgPrec\n",
    "    avgprec = sklearn_metrics.label_ranking_average_precision_score(y_gt, y_pred)\n",
    "    # Compute Label Ranking Loss\n",
    "    rankloss = sklearn_metrics.label_ranking_loss(y_gt, y_pred)\n",
    "    # Compute Precision, Recall and F1 score\n",
    "    precision, recall, f1, _ = sklearn_metrics.precision_recall_fscore_support(y_gt, y_pred, average='micro')\n",
    "\n",
    "    if verbose > 0:\n",
    "        logger.info(\n",
    "            '\"coverage_error\" (best: avg labels per sample = %f): %f' % (\n",
    "                float(np.sum(y_gt)) / float(n_samples), coverr))\n",
    "        logger.info('Label Ranking \"average_precision\" (best: 1.0): %f' % avgprec)\n",
    "        logger.info('Label \"ranking_loss\" (best: 0.0): %f' % rankloss)\n",
    "        logger.info('precision: %f' % precision)\n",
    "        logger.info('recall: %f' % recall)\n",
    "        logger.info('f1: %f' % f1)\n",
    "\n",
    "    return {'coverage_error': coverr,\n",
    "            'average_precision': avgprec,\n",
    "            'ranking_loss': rankloss,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import coverage_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.confusion = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1s = []\n",
    "        self.kappa = []\n",
    "        self.auc = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        score = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        predict = np.round(np.asarray(self.model.predict(self.validation_data[0])))\n",
    "        targ = self.validation_data[1]\n",
    "\n",
    "        self.auc.append(sklm.roc_auc_score(targ, score))\n",
    "        self.confusion.append(sklm.confusion_matrix(targ, predict))\n",
    "        self.precision.append(sklm.precision_score(targ, predict))\n",
    "        self.recall.append(sklm.recall_score(targ, predict))\n",
    "        self.f1s.append(sklm.f1_score(targ, predict))\n",
    "        self.kappa.append(sklm.cohen_kappa_score(targ, predict))\n",
    "\n",
    "        return\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def multilabel_metrics(y_true, y_pred):\n",
    "    print(y_true, y_pred)\n",
    "    print(type(y_true))\n",
    "    return 10\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_metrics\n",
      "  Downloading keras_metrics-1.1.0-py2.py3-none-any.whl (5.6 kB)\n",
      "Requirement already satisfied: Keras>=2.1.5 in c:\\users\\melvi\\anaconda3\\lib\\site-packages (from keras_metrics) (2.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\melvi\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\melvi\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\melvi\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (1.18.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\melvi\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (5.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\melvi\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (1.1.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\melvi\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (1.14.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\melvi\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (2.10.0)\n",
      "Installing collected packages: keras-metrics\n",
      "Successfully installed keras-metrics-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\t\n",
    "    \"\"\"Precision metric.\t\n",
    "    Only computes a batch-wise average of precision.\t\n",
    "    Computes the precision, a metric for multi-label classification of\t\n",
    "    how many selected items are relevant.\t\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\t\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\t\n",
    "    return precision\t\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\t\n",
    "    \"\"\"Recall metric.\t\n",
    "    Only computes a batch-wise average of recall.\t\n",
    "    Computes the recall, a metric for multi-label classification of\t\n",
    "    how many relevant items are selected.\t\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\t\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\t\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta_score(y_true, y_pred, beta=1):\t\n",
    "    \"\"\"Computes the F score.\t\n",
    "    The F score is the weighted harmonic mean of precision and recall.\t\n",
    "    Here it is only computed as a batch-wise average, not globally.\t\n",
    "    This is useful for multi-label classification, where input samples can be\t\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\t\n",
    "    would achieve a perfect score by simply assigning every class to every\t\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\t\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\t\n",
    "    computes this, as a weighted mean of the proportion of correct class\t\n",
    "    assignments vs. the proportion of incorrect class assignments.\t\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\t\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\t\n",
    "    instead weighted towards penalizing incorrect class assignments.\t\n",
    "    \"\"\"\t\n",
    "    if beta < 0:\t\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\t\n",
    "#     print(tf.cond(K.sum(K.round(K.clip(y_true, 0, 1)))))\n",
    "\n",
    "    if tf.math.equal(K.sum(K.round(K.clip(y_true, 0, 1))), 0) is not None:\n",
    "        print('asdfiojasd')\n",
    "        return 0\t\n",
    "\n",
    "    p = precision(y_true, y_pred)\t\n",
    "    r = recall(y_true, y_pred)\t\n",
    "    bb = beta ** 2\t\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\t\n",
    "    return fbeta_score\t\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\t\n",
    "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\t\n",
    "    Here it is only computed as a batch-wise average, not globally.\t\n",
    "    \"\"\"\t\n",
    "    return fbeta_score(y_true, y_pred, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "69/69 [==============================] - 16s 230ms/step - loss: 0.0235 - f1_m: 0.1383 - val_loss: 0.0267 - val_f1_m: 0.1354\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.13544, saving model to output\\test/model.h5\n",
      "Epoch 2/25\n",
      "69/69 [==============================] - 18s 259ms/step - loss: 0.0233 - f1_m: 0.1389 - val_loss: 0.0210 - val_f1_m: 0.1316\n",
      "\n",
      "Epoch 00002: val_f1_m did not improve from 0.13544\n",
      "Epoch 3/25\n",
      "69/69 [==============================] - 16s 239ms/step - loss: 0.0234 - f1_m: 0.1347 - val_loss: 0.0239 - val_f1_m: 0.1315\n",
      "\n",
      "Epoch 00003: val_f1_m did not improve from 0.13544\n",
      "Epoch 4/25\n",
      "69/69 [==============================] - 19s 282ms/step - loss: 0.0233 - f1_m: 0.1367 - val_loss: 0.0214 - val_f1_m: 0.1413\n",
      "\n",
      "Epoch 00004: val_f1_m improved from 0.13544 to 0.14133, saving model to output\\test/model.h5\n",
      "Epoch 5/25\n",
      "69/69 [==============================] - 19s 273ms/step - loss: 0.0233 - f1_m: 0.1394 - val_loss: 0.0266 - val_f1_m: 0.1281\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00005: val_f1_m did not improve from 0.14133\n",
      "Epoch 6/25\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.0233 - f1_m: 0.1366 - val_loss: 0.0242 - val_f1_m: 0.1299\n",
      "\n",
      "Epoch 00006: val_f1_m did not improve from 0.14133\n",
      "Epoch 7/25\n",
      "69/69 [==============================] - 20s 289ms/step - loss: 0.0230 - f1_m: 0.1411 - val_loss: 0.0238 - val_f1_m: 0.1448\n",
      "\n",
      "Epoch 00007: val_f1_m improved from 0.14133 to 0.14476, saving model to output\\test/model.h5\n",
      "Epoch 8/25\n",
      "69/69 [==============================] - 18s 263ms/step - loss: 0.0234 - f1_m: 0.1337 - val_loss: 0.0257 - val_f1_m: 0.1227\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00008: val_f1_m did not improve from 0.14476\n",
      "Epoch 9/25\n",
      "69/69 [==============================] - 18s 266ms/step - loss: 0.0231 - f1_m: 0.1378 - val_loss: 0.0280 - val_f1_m: 0.1326\n",
      "\n",
      "Epoch 00009: val_f1_m did not improve from 0.14476\n",
      "Epoch 10/25\n",
      "69/69 [==============================] - 19s 270ms/step - loss: 0.0232 - f1_m: 0.1413 - val_loss: 0.0271 - val_f1_m: 0.1464\n",
      "\n",
      "Epoch 00010: val_f1_m improved from 0.14476 to 0.14645, saving model to output\\test/model.h5\n",
      "Epoch 11/25\n",
      "69/69 [==============================] - 18s 259ms/step - loss: 0.0230 - f1_m: 0.1360 - val_loss: 0.0234 - val_f1_m: 0.1202\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00011: val_f1_m did not improve from 0.14645\n",
      "Epoch 12/25\n",
      "69/69 [==============================] - 18s 258ms/step - loss: 0.0230 - f1_m: 0.1379 - val_loss: 0.0202 - val_f1_m: 0.1382\n",
      "\n",
      "Epoch 00012: val_f1_m did not improve from 0.14645\n",
      "Epoch 13/25\n",
      "69/69 [==============================] - 19s 268ms/step - loss: 0.0230 - f1_m: 0.1380 - val_loss: 0.0234 - val_f1_m: 0.1417\n",
      "\n",
      "Epoch 00013: val_f1_m did not improve from 0.14645\n",
      "Epoch 14/25\n",
      "69/69 [==============================] - 18s 259ms/step - loss: 0.0232 - f1_m: 0.1374 - val_loss: 0.0238 - val_f1_m: 0.1205\n",
      "\n",
      "Epoch 00014: val_f1_m did not improve from 0.14645\n",
      "Epoch 15/25\n",
      "69/69 [==============================] - 19s 273ms/step - loss: 0.0230 - f1_m: 0.1353 - val_loss: 0.0243 - val_f1_m: 0.1377\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00015: val_f1_m did not improve from 0.14645\n",
      "Epoch 16/25\n",
      "69/69 [==============================] - 21s 299ms/step - loss: 0.0234 - f1_m: 0.1360 - val_loss: 0.0194 - val_f1_m: 0.1395\n",
      "\n",
      "Epoch 00016: val_f1_m did not improve from 0.14645\n",
      "Epoch 17/25\n",
      "69/69 [==============================] - 19s 282ms/step - loss: 0.0226 - f1_m: 0.1407 - val_loss: 0.0279 - val_f1_m: 0.1227\n",
      "\n",
      "Epoch 00017: val_f1_m did not improve from 0.14645\n",
      "Epoch 18/25\n",
      "69/69 [==============================] - 20s 287ms/step - loss: 0.0231 - f1_m: 0.1356 - val_loss: 0.0225 - val_f1_m: 0.1383\n",
      "\n",
      "Epoch 00018: val_f1_m did not improve from 0.14645\n",
      "Epoch 19/25\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.0232 - f1_m: 0.1386 - val_loss: 0.0194 - val_f1_m: 0.1359\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00019: val_f1_m did not improve from 0.14645\n",
      "Epoch 20/25\n",
      "69/69 [==============================] - 19s 278ms/step - loss: 0.0228 - f1_m: 0.1395 - val_loss: 0.0177 - val_f1_m: 0.1277\n",
      "\n",
      "Epoch 00020: val_f1_m did not improve from 0.14645\n",
      "Epoch 21/25\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.0229 - f1_m: 0.1375 - val_loss: 0.0266 - val_f1_m: 0.1354\n",
      "\n",
      "Epoch 00021: val_f1_m did not improve from 0.14645\n",
      "Epoch 22/25\n",
      "69/69 [==============================] - 18s 268ms/step - loss: 0.0231 - f1_m: 0.1374 - val_loss: 0.0210 - val_f1_m: 0.1321\n",
      "\n",
      "Epoch 00022: val_f1_m did not improve from 0.14645\n",
      "Epoch 23/25\n",
      "69/69 [==============================] - 19s 280ms/step - loss: 0.0227 - f1_m: 0.1384 - val_loss: 0.0233 - val_f1_m: 0.1315\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00023: val_f1_m did not improve from 0.14645\n",
      "Epoch 24/25\n",
      "69/69 [==============================] - 20s 292ms/step - loss: 0.0231 - f1_m: 0.1374 - val_loss: 0.0212 - val_f1_m: 0.1413\n",
      "\n",
      "Epoch 00024: val_f1_m did not improve from 0.14645\n",
      "Epoch 25/25\n",
      "69/69 [==============================] - 19s 278ms/step - loss: 0.0230 - f1_m: 0.1395 - val_loss: 0.0265 - val_f1_m: 0.1281\n",
      "\n",
      "Epoch 00025: val_f1_m did not improve from 0.14645\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adadelta(lr = LEARNING_RATE)\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [f1_m])\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "                    monitor='val_loss', \n",
    "                    patience=PATIENCE, verbose=VERBOSE, \n",
    "                    factor=LEARNING_RATE_REDUCTION_FACTOR, \n",
    "                    min_lr=MIN_LEARNING_RATE\n",
    ")\n",
    "save_model = ModelCheckpoint(filepath=model_out_dir + \"/model.h5\", monitor='val_f1_m', verbose=VERBOSE, \n",
    "                             save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                                  epochs=EPOCHS,\n",
    "                                  steps_per_epoch=(train_generator.n // BATCH_SIZE) + 1, \n",
    "                                  verbose=VERBOSE,\n",
    "                                  validation_data=val_generator,\n",
    "                                  validation_steps=(val_generator.n // BATCH_SIZE) + 1,\n",
    "                                  callbacks=[learning_rate_reduction, save_model]\n",
    "                             )\n",
    "weights = model.load_weights(model_out_dir + \"/model.h5\")\n",
    "weights\n",
    "# validation_data=validation_gen,\n",
    "#                                   validation_steps=(val_generator.n // BATCH_SIZE) + 1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 62s 80ms/step\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(model_out_dir + \"/model.h5\")\n",
    "\n",
    "val_generator.reset()\n",
    "test_generator.reset()\n",
    "# loss_v, accuracy_v = model.evaluate_generator(val_generator, steps=(val_generator.n // BATCH_SIZE) + 1, verbose=VERBOSE)\n",
    "# loss, accuracy = model.evaluate_generator(test_generator, steps=(test_generator.n // BATCH_SIZE) + 1, verbose=VERBOSE)\n",
    "# print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\n",
    "# print(\"Test: accuracy = %f  ;  loss_v = %f\" % (accuracy, loss))\n",
    "\n",
    "# plot_model_history(history, out_path=model_out_dir)\n",
    "test_generator.reset()\n",
    "y_pred = model.predict_generator(test_generator, steps=test_generator.n, verbose=VERBOSE)\n",
    "# y_true = test_generator.classes\n",
    "# plot_confusion_matrix(y_true, y_pred.argmax(axis=-1), labels, out_path=model_out_dir)\n",
    "# class_report = classification_report(y_true, y_pred.argmax(axis=-1), target_names=labels)\n",
    "\n",
    "# with open(model_out_dir + \"/classification_report.txt\", \"w\") as text_file:\n",
    "#     text_file.write(\"%s\" % class_report)\n",
    "# print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "yFit = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-cd6e0835d24b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlabel_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlabel_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#flip k,v\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0myFit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-cd6e0835d24b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlabel_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlabel_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#flip k,v\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0myFit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "predictions = [label_map[k] for k in yFit]\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-200-6072cd5101c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not dict"
     ]
    }
   ],
   "source": [
    "y_true = test_generator.classes[test_generator.class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.6173203e-06, 5.8203254e-06, 5.8460037e-06, ..., 5.6422651e-03,\n",
       "        5.4481134e-06, 8.9728832e-04],\n",
       "       [6.4220671e-06, 6.6857824e-06, 6.6770144e-06, ..., 6.0545504e-03,\n",
       "        6.2732884e-06, 9.5963478e-04],\n",
       "       [3.2352291e-06, 3.3342071e-06, 3.3530812e-06, ..., 4.2439401e-03,\n",
       "        3.1260770e-06, 6.6253543e-04],\n",
       "       ...,\n",
       "       [6.5263112e-06, 6.0874622e-06, 5.5219839e-06, ..., 6.1878562e-04,\n",
       "        6.4187975e-06, 1.3121963e-03],\n",
       "       [2.1568851e-06, 1.9310496e-06, 1.8926436e-06, ..., 7.6565146e-04,\n",
       "        2.0240489e-06, 7.0631504e-04],\n",
       "       [1.0747797e-06, 1.0167128e-06, 1.0489837e-06, ..., 1.5169680e-03,\n",
       "        9.6812357e-07, 4.1061640e-04]], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.where(y_pred > 0.01, 1, 0)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    24498\n",
       "0       31\n",
       "Name: 9, dtype: int64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(test)\n",
    "df[9].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_generator(test_generator, verbose=VERBOSE)\n",
    "y_pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaas = y_pred > 0.5\n",
    "kaas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices = np.argmax(y_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.argmax(y_pred, axis=1)\n",
    "\n",
    "labels = (train_generator.class_indices)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history, out_path=\"\"):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1, len(model_history.history['accuracy']) + 1), model_history.history['accuracy'])\n",
    "    axs[0].plot(range(1, len(model_history.history['val_accuracy']) + 1), model_history.history['val_accuracy'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1, len(model_history.history['loss']) + 1), model_history.history['loss'])\n",
    "    axs[1].plot(range(1, len(model_history.history['val_loss']) + 1), model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    # save the graph in a file called \"acc_loss.png\" to be available for later; the model_name is provided when creating and training a model\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/acc_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, out_path=\"\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index=[i for i in classes], columns=[i for i in classes])\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    ax = sn.heatmap(df_cm, annot=True, square=True, fmt=\"d\", linewidths=.2, cbar_kws={\"shrink\": 0.8})\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/confusion_matrix.png\")  # as in the plot_model_history, the matrix is saved in a file called \"model_name_confusion_matrix.png\"\n",
    "    return ax    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-56a07aa5b3e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcoverage_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mcoverage_error\u001b[1;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m     \"\"\"\n\u001b[1;32m--> 942\u001b[1;33m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;31m# make sure we actually converted to numeric:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"O\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "coverage_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_generator(test_generator, steps=test_generator.n, verbose=VERBOSE)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
