{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Activation, Dropout, Lambda\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5883280757097792"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "373/634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_RESNET = False\n",
    "\n",
    "INPUT_SHAPE = (100, 100, 3)\n",
    "TARGET_SIZE = (100, 100)\n",
    "NUM_CLASSES =  101\n",
    "LEARNING_RATE = 0.0001\n",
    "PATIENCE = 3\n",
    "VERBOSE = 1\n",
    "LEARNING_RATE_REDUCTION_FACTOR = 0.5\n",
    "MIN_LEARNING_RATE = 0.00001\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "MODEL_NAME = 'category_model'\n",
    "OUTPUT_DIR = 'output'\n",
    "SEED = 42\n",
    "\n",
    "MODEL_OUT_DIR = os.path.join(OUTPUT_DIR, MODEL_NAME)\n",
    "                            \n",
    "if not os.path.exists(MODEL_OUT_DIR):\n",
    "    os.makedirs(MODEL_OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled = pd.read_csv('data/train_labeled.csv')\n",
    "test_labeled = pd.read_csv('data/test_labeled.csv')\n",
    "val_labeled = pd.read_csv('data/val_labeled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_process(x):\n",
    "    import tensorflow as tf\n",
    "    return x\n",
    "    hsv = tf.image.rgb_to_hsv(x)\n",
    "    gray = tf.image.rgb_to_grayscale(x)\n",
    "    rez = tf.concat([hsv, gray], axis=-1)\n",
    "    return rez\n",
    "\n",
    "def network(input_shape, num_classes):\n",
    "    img_input = Input(shape=input_shape, name='data')\n",
    "    x = Lambda(image_process)(img_input)\n",
    "    x = Conv2D(16, (5, 5), strides=(1, 1), padding='same', name='conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool1')(x)\n",
    "    x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', name='conv2')(x)\n",
    "    x = Activation('relu', name='conv2_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool2')(x)\n",
    "    x = Conv2D(64, (5, 5), strides=(1, 1), padding='same', name='conv3')(x)\n",
    "    x = Activation('relu', name='conv3_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool3')(x)\n",
    "    x = Conv2D(128, (5, 5), strides=(1, 1), padding='same', name='conv4')(x)\n",
    "    x = Activation('relu', name='conv4_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool4')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu', name='fcl1')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu', name='fcl2')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    rez = Model(inputs=img_input, outputs=out)\n",
    "    return rez\n",
    "\n",
    "def resnet(input_shape, num_classes):\n",
    "    resnet = ResNet50(weights='imagenet',\n",
    "                       input_shape=input_shape,\n",
    "                   include_top=False)\n",
    "    image = resnet.get_layer(index=0).output\n",
    "    output = resnet.get_layer(index=-1).output\n",
    "    output = Flatten()(output)\n",
    "#     output = Dense(num_classes, activation='sigmoid', name='predictions')(output)\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    resnet = Model(inputs = image, outputs = output)\n",
    "    model = Sequential()\n",
    "    model.add(resnet)\n",
    "    model.add(Dense(512, activation='relu', input_dim=input_shape))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='sigmoid', name='predictions'))\n",
    "    model.summary();\n",
    "    \n",
    "    return model\n",
    "    \n",
    "if USE_RESNET:\n",
    "    model = resnet(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "else:\n",
    "    model = network(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "# model = resnet(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "# model = resnet(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3814 validated image filenames belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.0,\n",
    "        height_shift_range=0.0,\n",
    "        zoom_range=0.0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,  # randomly flip images\n",
    "        rescale=1./255.\n",
    ")\n",
    "\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_labeled,\n",
    "    directory=\"./Recipes5k/images/\",\n",
    "    x_col=\"url\",\n",
    "    y_col=\"category\",\n",
    "    subset=\"training\",\n",
    "    batch_size=32,\n",
    "    seed=SEED,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "#     classes=['Vegetarian', 'Non-Vegetarian'],\n",
    "#     classes={'Non-Vegetarian':1, 'Vegetarian':0},\n",
    "    target_size=TARGET_SIZE\n",
    ")\n",
    "labels = train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 783 validated image filenames belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255.\n",
    ")\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_labeled,\n",
    "    directory=\"./Recipes5k/images/\",\n",
    "    x_col=\"url\",\n",
    "    y_col=\"category\",\n",
    "    batch_size=1,\n",
    "    seed=SEED,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=TARGET_SIZE,\n",
    "    classes = labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.0,\n",
    "        height_shift_range=0.0,\n",
    "        zoom_range=0.0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,  # randomly flip images\n",
    "        rescale=1./255.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 634 validated image filenames belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_labeled,\n",
    "    directory=\"./Recipes5k/images/\",\n",
    "    x_col=\"url\",\n",
    "    y_col=\"category\",\n",
    "    batch_size=32,\n",
    "    seed=SEED,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=TARGET_SIZE,\n",
    "    classes = labels\n",
    ")\n",
    "# val_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred):\n",
    "    gamma = 2.0\n",
    "    alpha = 0.25\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07892504, 0.78671617, 1.07892504, 1.34865629, 0.8991042 ,\n",
       "       0.99374674, 1.07892504, 1.07892504, 0.82092122, 1.07892504,\n",
       "       1.07892504, 1.07892504, 0.8781948 , 1.14431443, 1.07892504,\n",
       "       1.07892504, 0.94405941, 1.07892504, 1.07892504, 0.85823582,\n",
       "       0.8781948 , 0.83916392, 1.07892504, 1.07892504, 1.07892504,\n",
       "       1.07892504, 0.92103357, 0.8781948 , 0.94405941, 1.07892504,\n",
       "       0.83916392, 1.07892504, 1.07892504, 1.11065812, 1.07892504,\n",
       "       1.07892504, 0.80345481, 1.98749349, 1.07892504, 0.8781948 ,\n",
       "       0.94405941, 0.8991042 , 1.07892504, 0.8781948 , 1.07892504,\n",
       "       1.39860653, 0.82092122, 1.11065812, 1.07892504, 1.07892504,\n",
       "       1.07892504, 1.07892504, 0.8991042 , 1.07892504, 0.92103357,\n",
       "       0.72619954, 0.80345481, 0.78671617, 1.07892504, 1.07892504,\n",
       "       0.92103357, 0.82092122, 1.64184245, 1.07892504, 1.07892504,\n",
       "       1.07892504, 1.14431443, 1.07892504, 1.11065812, 1.07892504,\n",
       "       0.94405941, 0.75524752, 0.80345481, 1.07892504, 1.07892504,\n",
       "       1.11065812, 0.83916392, 1.07892504, 0.80345481, 0.8991042 ,\n",
       "       1.07892504, 0.92103357, 0.85823582, 1.07892504, 1.07892504,\n",
       "       1.07892504, 1.07892504, 0.85823582, 1.07892504, 1.11065812,\n",
       "       0.94405941, 0.82092122, 0.77066074, 1.07892504, 0.78671617,\n",
       "       1.07892504, 0.8991042 , 6.29372937, 1.07892504, 1.11065812,\n",
       "       1.07892504])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_labeled['category']),\n",
    "                                                 train_labeled['category'])\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "77/77 [==============================] - 16s 203ms/step - loss: 4.6174 - accuracy: 0.0077 - val_loss: 4.6514 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.00000, saving model to output\\category_model/model.h5\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 16s 208ms/step - loss: 4.6144 - accuracy: 0.0103 - val_loss: 4.6320 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.00000\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 17s 219ms/step - loss: 4.6112 - accuracy: 0.0107 - val_loss: 4.6278 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.00000\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 17s 220ms/step - loss: 4.5963 - accuracy: 0.0130 - val_loss: 4.6392 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.00000\n",
      "Epoch 5/10\n",
      "53/77 [===================>..........] - ETA: 5s - loss: 4.5869 - accuracy: 0.0192"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-aabbdbff9138>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                                   \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                                   \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlearning_rate_reduction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                              )\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# weights = model.load_weights(MODEL_OUT_DIR + \"/model.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \"\"\"\n\u001b[1;32m-> 1605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = Adam(lr = LEARNING_RATE)\n",
    "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "                    monitor='val_loss', \n",
    "                    patience=PATIENCE, verbose=VERBOSE, \n",
    "                    factor=LEARNING_RATE_REDUCTION_FACTOR, \n",
    "                    min_lr=MIN_LEARNING_RATE\n",
    ")\n",
    "save_model = ModelCheckpoint(filepath=MODEL_OUT_DIR + \"/model.h5\", monitor='val_accuracy', verbose=VERBOSE, \n",
    "                             save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                                  epochs=EPOCHS,\n",
    "                                  steps_per_epoch=(train_generator.n // BATCH_SIZE) + 1, \n",
    "                                  verbose=VERBOSE,\n",
    "                                  validation_data=val_generator,\n",
    "                                  validation_steps=(val_generator.n // BATCH_SIZE) + 1,\n",
    "                                  callbacks=[learning_rate_reduction, save_model]\n",
    "                             )\n",
    "# weights = model.load_weights(MODEL_OUT_DIR + \"/model.h5\")\n",
    "# weights\n",
    "# validation_data=validation_gen,\n",
    "#                                   validation_steps=(val_generator.n // BATCH_SIZE) + 1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history, out_path=\"\"):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1, len(model_history.history['accuracy']) + 1), model_history.history['accuracy'])\n",
    "    axs[0].plot(range(1, len(model_history.history['val_accuracy']) + 1), model_history.history['val_accuracy'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1, len(model_history.history['loss']) + 1), model_history.history['loss'])\n",
    "    axs[1].plot(range(1, len(model_history.history['val_loss']) + 1), model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    # save the graph in a file called \"acc_loss.png\" to be available for later; the model_name is provided when creating and training a model\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/acc_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, out_path=\"\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index=[i for i in classes], columns=[i for i in classes])\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    ax = sn.heatmap(df_cm, annot=True, square=True, fmt=\"d\", linewidths=.2, cbar_kws={\"shrink\": 0.8})\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/confusion_matrix.png\")  # as in the plot_model_history, the matrix is saved in a file called \"model_name_confusion_matrix.png\"\n",
    "    return ax    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(MODEL_OUT_DIR + \"/model.h5\")\n",
    "\n",
    "val_generator.reset()\n",
    "test_generator.reset()\n",
    "# loss_v, accuracy_v = model.evaluate_generator(val_generator, steps=(val_generator.n // BATCH_SIZE) + 1, verbose=VERBOSE)\n",
    "# loss, accuracy = model.evaluate_generator(test_generator, steps=(test_generator.n // BATCH_SIZE) + 1, verbose=VERBOSE)\n",
    "# print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\n",
    "# print(\"Test: accuracy = %f  ;  loss_v = %f\" % (accuracy, loss))\n",
    "\n",
    "# plot_model_history(history, out_path=MODEL_OUT_DIR)\n",
    "test_generator.reset()\n",
    "y_pred = model.predict_generator(test_generator, steps=test_generator.n, verbose=VERBOSE)\n",
    "# test = np.where(y_pred > 0.50, 1, 0)\n",
    "y_true = test_generator.classes\n",
    "# plot_confusion_matrix(y_true, y_pred.argmax(axis=-1), None, out_path=MODEL_OUT_DIR)\n",
    "# class_report = classification_report(y_true, y_pred.argmax(axis=-1), target_names=labels)\n",
    "\n",
    "# with open(MODEL_OUT_DIR + \"/classification_report.txt\", \"w\") as text_file:\n",
    "#     text_file.write(\"%s\" % class_report)\n",
    "# print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = pd.read_csv('data/category_not_vegetarian.csv', index_col='category')\n",
    "category_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "labels = {value:key for key, value in test_generator.class_indices.items()}\n",
    "y_pred_labels = np.vectorize(labels.get)(y_pred.argmax(axis=-1))\n",
    "y_pred_binary = [category_labels.loc[pred]['vegetarian'] for pred in y_pred_labels]\n",
    "\n",
    "y_true_labels = np.vectorize(labels.get)(y_true)\n",
    "y_true_binary = [category_labels.loc[pred]['vegetarian'] for pred in y_true_labels]\n",
    "\n",
    "\n",
    "accuracy_score(y_true_binary, y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true_binary, y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true_binary, y_pred_binary, None, out_path=MODEL_OUT_DIR)\n",
    "class_report = classification_report(y_true_binary, y_pred_binary, target_names=labels)\n",
    "\n",
    "with open(MODEL_OUT_DIR + \"/classification_report.txt\", \"w\") as text_file:\n",
    "    text_file.write(\"%s\" % class_report)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(y_true,y_pred.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_true)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.where(y_pred > 0.50, 1, 0)\n",
    "test;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labeled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "492/783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred < 0.55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_generator(test_generator, verbose=VERBOSE)\n",
    "y_pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaas = y_pred > 0.5\n",
    "kaas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices = np.argmax(y_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.argmax(y_pred, axis=1)\n",
    "\n",
    "labels = (train_generator.class_indices)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history, out_path=\"\"):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1, len(model_history.history['accuracy']) + 1), model_history.history['accuracy'])\n",
    "    axs[0].plot(range(1, len(model_history.history['val_accuracy']) + 1), model_history.history['val_accuracy'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1, len(model_history.history['loss']) + 1), model_history.history['loss'])\n",
    "    axs[1].plot(range(1, len(model_history.history['val_loss']) + 1), model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    # save the graph in a file called \"acc_loss.png\" to be available for later; the model_name is provided when creating and training a model\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/acc_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, out_path=\"\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index=[i for i in classes], columns=[i for i in classes])\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    ax = sn.heatmap(df_cm, annot=True, square=True, fmt=\"d\", linewidths=.2, cbar_kws={\"shrink\": 0.8})\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/confusion_matrix.png\")  # as in the plot_model_history, the matrix is saved in a file called \"model_name_confusion_matrix.png\"\n",
    "    return ax    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_generator(test_generator, steps=test_generator.n, verbose=VERBOSE)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
